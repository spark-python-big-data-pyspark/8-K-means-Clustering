{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5853a993",
   "metadata": {},
   "source": [
    "# Clustering Code Along"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed697b29",
   "metadata": {},
   "source": [
    "The examined group comprised kernels belonging to three different varieties of wheat: Kama, Rosa and Canadian, 70 elements each, randomly selected for \n",
    "the experiment. High quality visualization of the internal kernel structure was detected using a soft X-ray technique. It is non-destructive and considerably cheaper than other more sophisticated imaging techniques like scanning microscopy or laser technology. The images were recorded on 13x18 cm X-ray KODAK plates. Studies were conducted using combine harvested wheat grain originating from experimental fields, explored at the Institute of Agrophysics of the Polish Academy of Sciences in Lublin. \n",
    "\n",
    "The data set can be used for the tasks of classification and cluster analysis.\n",
    "\n",
    "\n",
    "Attribute Information:\n",
    "\n",
    "To construct the data, seven geometric parameters of wheat kernels were measured: \n",
    "1. area A, \n",
    "2. perimeter P, \n",
    "3. compactness C = 4*pi*A/P^2, \n",
    "4. length of kernel, \n",
    "5. width of kernel, \n",
    "6. asymmetry coefficient \n",
    "7. length of kernel groove. \n",
    "All of these parameters were real-valued continuous.\n",
    "\n",
    "Let's see if we can cluster them in to 3 groups with K-means!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a05ce17c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName('cluster').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "daaa2208",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.clustering import KMeans\n",
    "\n",
    "# Loads data.\n",
    "dataset = spark.read.csv(\"./data/seeds_dataset.csv\",header=True,inferSchema=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "49960910",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "86025bc9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+\n",
      "| area|perimeter|compactness|  length_of_kernel|   width_of_kernel|asymmetry_coefficient|  length_of_groove|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+\n",
      "|15.26|    14.84|      0.871|             5.763|             3.312|                2.221|              5.22|\n",
      "|14.88|    14.57|     0.8811| 5.553999999999999|             3.333|                1.018|             4.956|\n",
      "|14.29|    14.09|      0.905|             5.291|3.3369999999999997|                2.699|             4.825|\n",
      "|13.84|    13.94|     0.8955|             5.324|3.3789999999999996|                2.259|             4.805|\n",
      "|16.14|    14.99|     0.9034|5.6579999999999995|             3.562|                1.355|             5.175|\n",
      "|14.38|    14.21|     0.8951|             5.386|             3.312|   2.4619999999999997|             4.956|\n",
      "|14.69|    14.49|     0.8799|             5.563|             3.259|   3.5860000000000003| 5.218999999999999|\n",
      "|14.11|     14.1|     0.8911|              5.42|             3.302|                  2.7|               5.0|\n",
      "|16.63|    15.46|     0.8747|             6.053|             3.465|                 2.04| 5.877000000000001|\n",
      "|16.44|    15.25|      0.888|5.8839999999999995|             3.505|                1.969|5.5329999999999995|\n",
      "|15.26|    14.85|     0.8696|5.7139999999999995|             3.242|                4.543|             5.314|\n",
      "|14.03|    14.16|     0.8796|             5.438|             3.201|   1.7169999999999999|             5.001|\n",
      "|13.89|    14.02|      0.888|             5.439|             3.199|                3.986|             4.738|\n",
      "|13.78|    14.06|     0.8759|             5.479|             3.156|                3.136|             4.872|\n",
      "|13.74|    14.05|     0.8744|             5.482|             3.114|                2.932|             4.825|\n",
      "|14.59|    14.28|     0.8993|             5.351|             3.333|                4.185| 4.781000000000001|\n",
      "|13.99|    13.83|     0.9183|             5.119|             3.383|                5.234| 4.781000000000001|\n",
      "|15.69|    14.75|     0.9058|             5.527|             3.514|                1.599|             5.046|\n",
      "| 14.7|    14.21|     0.9153|             5.205|             3.466|                1.767|             4.649|\n",
      "|12.72|    13.57|     0.8686|             5.226|             3.049|                4.102|             4.914|\n",
      "+-----+---------+-----------+------------------+------------------+---------------------+------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dataset.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "212ff57e",
   "metadata": {},
   "source": [
    "## Format the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2fd95ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import VectorAssembler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "af60a835",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['area',\n",
       " 'perimeter',\n",
       " 'compactness',\n",
       " 'length_of_kernel',\n",
       " 'width_of_kernel',\n",
       " 'asymmetry_coefficient',\n",
       " 'length_of_groove']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7b47bed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "assembler = VectorAssembler(inputCols = dataset.columns,outputCol= 'features')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "11ecf460",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = assembler.transform(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "38785693",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- area: double (nullable = true)\n",
      " |-- perimeter: double (nullable = true)\n",
      " |-- compactness: double (nullable = true)\n",
      " |-- length_of_kernel: double (nullable = true)\n",
      " |-- width_of_kernel: double (nullable = true)\n",
      " |-- asymmetry_coefficient: double (nullable = true)\n",
      " |-- length_of_groove: double (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Exception ignored in: <function JavaWrapper.__del__ at 0x7f324419ea60>\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/patricia/miniconda3/envs/spark-course/lib/python3.8/site-packages/pyspark/ml/wrapper.py\", line 42, in __del__\n",
      "    if SparkContext._active_spark_context and self._java_obj is not None:\n",
      "AttributeError: 'VectorAssembler' object has no attribute '_java_obj'\n"
     ]
    }
   ],
   "source": [
    "final_data.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d05c9d",
   "metadata": {},
   "source": [
    "## Scale the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2dd92af",
   "metadata": {},
   "source": [
    "En PySpark, StandardScaler es una clase que se utiliza para realizar la estandarización de características (también conocida como normalización z-score) en un conjunto de datos. La estandarización es un paso común en el preprocesamiento de datos antes de aplicar algoritmos de aprendizaje automático, ya que puede ayudar a que las características tengan escalas comparables y facilitar así el entrenamiento del modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b4fe78a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d9d89f8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler(inputCol ='features',outputCol='scaledFeatures')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bd7f85b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler_model = scaler.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6db29b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_data = scaler_model.transform(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "313f085b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(area=15.26, perimeter=14.84, compactness=0.871, length_of_kernel=5.763, width_of_kernel=3.312, asymmetry_coefficient=2.221, length_of_groove=5.22, features=DenseVector([15.26, 14.84, 0.871, 5.763, 3.312, 2.221, 5.22]), scaledFeatures=DenseVector([5.2445, 11.3633, 36.8608, 13.0072, 8.7685, 1.4772, 10.621]))]"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_data.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00ad30e9",
   "metadata": {},
   "source": [
    "## Train the Model and Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8f5266a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "kmeans = KMeans(featuresCol ='scaledFeatures',k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "aed2d433",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = kmeans.fit(final_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dc42d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import ClusteringEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "30a01e58",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Silhouette with squared euclidean distance = 0.6300001033389961\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Predecir las asignaciones de clúster para los datos\n",
    "predictions = model.transform(final_data)\n",
    "\n",
    "# Calcular la suma de los cuadrados de las distancias\n",
    "evaluator = ClusteringEvaluator()\n",
    "silhouette = evaluator.evaluate(predictions)\n",
    "print(\"Silhouette with squared euclidean distance = \" + str(silhouette))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "d147b560",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+\n",
      "|prediction|\n",
      "+----------+\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         1|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         2|\n",
      "|         0|\n",
      "+----------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model.transform(final_data).select('prediction').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "09e5a913",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cluster Centers: \n",
      "[ 4.07497225 10.14410142 35.89816849 11.80812742  7.54416916  3.15410901\n",
      " 10.38031464]\n",
      "[ 6.35645488 12.40730852 37.41990178 13.93860446  9.7892399   2.41585013\n",
      " 12.29286107]\n",
      "[ 4.96198582 10.97871333 37.30930808 12.44647267  8.62880781  1.80061978\n",
      " 10.41913733]\n"
     ]
    }
   ],
   "source": [
    "# Shows the result.\n",
    "centers = model.clusterCenters()\n",
    "print(\"Cluster Centers: \")\n",
    "for center in centers:\n",
    "    print(center)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
